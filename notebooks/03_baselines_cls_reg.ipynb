{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f33339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HGB_ALL_f23 ===\n",
      "AP macro (test): 0.4347\n",
      "AP por clase (test): {'breakout': 0.3376, 'none': 0.3138, 'rebound': 0.6525}\n",
      "Brier OvR medio (test): 0.19268\n",
      "CM (test):\n",
      "[[  93   21  601]\n",
      " [  63   40  642]\n",
      " [  69   40 1620]]\n",
      "Guardado: models/artifacts/HGB_ALL_f23_calibrated_isotonic.joblib\n",
      "\n",
      "=== HGB_EUUSA_f23 ===\n",
      "AP macro (test): 0.4231\n",
      "AP por clase (test): {'breakout': 0.3831, 'none': 0.2436, 'rebound': 0.6427}\n",
      "Brier OvR medio (test): 0.19695\n",
      "CM (test):\n",
      "[[ 35   2 469]\n",
      " [ 13   0 389]\n",
      " [ 15   3 972]]\n",
      "Guardado: models/artifacts/HGB_EUUSA_f23_calibrated_isotonic.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'name': 'HGB_ALL_f23',\n",
       "  'counts_train': {'rebound': 10696, 'none': 4726, 'breakout': 4104},\n",
       "  'ap_macro_test': 0.434655856245721,\n",
       "  'ap_per_class_test': {'breakout': 0.337646328576001,\n",
       "   'none': 0.3138268531590387,\n",
       "   'rebound': 0.6524943870021234},\n",
       "  'brier_test': 0.1926751684104424,\n",
       "  'cm_test': [[93, 21, 601], [63, 40, 642], [69, 40, 1620]]},\n",
       " {'name': 'HGB_EUUSA_f23',\n",
       "  'counts_train': {'rebound': 6232, 'breakout': 2958, 'none': 2623},\n",
       "  'ap_macro_test': 0.4231125213905547,\n",
       "  'ap_per_class_test': {'breakout': 0.3830821263378881,\n",
       "   'none': 0.2435837823778073,\n",
       "   'rebound': 0.6426716554559688},\n",
       "  'brier_test': 0.19695132128305023,\n",
       "  'cm_test': [[35, 2, 469], [13, 0, 389], [15, 3, 972]]})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 0) Imports ===\n",
    "import os, json, joblib\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, brier_score_loss, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# === 1) Carga df base (para el split temporal) ===\n",
    "DF_PATH = \"data/interim/ES_5m_2021_2024.parquet\"\n",
    "df = pd.read_parquet(DF_PATH)\n",
    "t_series = df[\"Time\"].dt.tz_convert(\"Europe/Madrid\")  # para mapping idx→fecha\n",
    "\n",
    "def split_by_time(X_idx):\n",
    "    t = t_series.loc[X_idx]\n",
    "    train = (t < \"2024-01-01\")\n",
    "    val   = (t >= \"2024-01-01\") & (t < \"2024-07-01\")\n",
    "    test  = (t >= \"2024-07-01\")\n",
    "    return train.values, val.values, test.values\n",
    "\n",
    "# === 2) utilidades métricas ===\n",
    "def ap_macro(y_true, proba, classes):\n",
    "    # y_true string -> binarizado OvR\n",
    "    Y = np.zeros((len(y_true), len(classes)), dtype=int)\n",
    "    for i,c in enumerate(classes):\n",
    "        Y[:,i] = (y_true == c).astype(int)\n",
    "    aps = []\n",
    "    for i,_ in enumerate(classes):\n",
    "        aps.append(average_precision_score(Y[:,i], proba[:,i]))\n",
    "    return float(np.mean(aps)), dict(zip(classes, aps))\n",
    "\n",
    "def eval_report(y_true, proba, classes):\n",
    "    y_pred = classes[proba.argmax(1)]\n",
    "    ap_mac, ap_per_class = ap_macro(y_true, proba, classes)\n",
    "    # Brier (multiclase: media de Brier OvR)\n",
    "    Y = np.zeros((len(y_true), len(classes)))\n",
    "    for i,c in enumerate(classes):\n",
    "        Y[:,i] = (y_true == c).astype(int)\n",
    "    brier = float(np.mean((proba - Y)**2))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(classes))\n",
    "    clf_rep = classification_report(y_true, y_pred, labels=list(classes), target_names=list(classes), output_dict=True)\n",
    "    return {\n",
    "        \"ap_macro\": ap_mac,\n",
    "        \"ap_per_class\": ap_per_class,\n",
    "        \"brier_ovr_mean\": brier,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"classification_report\": clf_rep,\n",
    "    }\n",
    "\n",
    "# === 3) entrenamiento + calibración + evaluación ===\n",
    "def train_eval_one(DATA_PATH, name, random_state=42):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    Path(\"models/artifacts\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # cargar dataset (X + target)\n",
    "    D = pd.read_parquet(DATA_PATH)\n",
    "    y_str = D[\"target\"].astype(str).values\n",
    "    X = D.drop(columns=[\"target\", \"idx\", \"zone_type\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # asegurar tipos numéricos y rellenar NaNs\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == \"bool\": X[c] = X[c].astype(np.int8)\n",
    "    num = X.select_dtypes(include=[np.number]).columns\n",
    "    X[num] = X[num].astype(np.float32)\n",
    "    X[num] = X[num].fillna(0.0)\n",
    "\n",
    "    # split temporal\n",
    "    idx_series = D[\"idx\"].astype(int) if \"idx\" in D.columns else None\n",
    "    if idx_series is None:\n",
    "        raise ValueError(\"El dataset debe contener la columna 'idx' para mapear a fechas.\")\n",
    "    tr, va, te = split_by_time(idx_series)\n",
    "\n",
    "    Xtr, ytr = X.loc[tr].to_numpy(), y_str[tr]\n",
    "    Xva, yva = X.loc[va].to_numpy(), y_str[va]\n",
    "    Xte, yte = X.loc[te].to_numpy(), y_str[te]\n",
    "\n",
    "    # clases y sample_weight por clase (inverso de la frecuencia en train)\n",
    "    classes = np.unique(ytr)\n",
    "    counts = pd.Series(ytr).value_counts()\n",
    "    inv_freq = counts.sum() / (len(counts) * counts)\n",
    "    sw_map = inv_freq.to_dict()\n",
    "    sw_tr = np.array([sw_map[y] for y in ytr], dtype=np.float32)\n",
    "\n",
    "    # modelo base\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.06,\n",
    "        max_depth=None,\n",
    "        max_leaf_nodes=31,\n",
    "        min_samples_leaf=50,\n",
    "        l2_regularization=0.0,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    hgb.fit(Xtr, ytr, sample_weight=sw_tr)\n",
    "\n",
    "    # calibración (isotónica) sobre VALIDACIÓN\n",
    "    calib = CalibratedClassifierCV(hgb, method=\"isotonic\", cv=\"prefit\")\n",
    "    calib.fit(Xva, yva)\n",
    "\n",
    "    # evaluación\n",
    "    proba_te = calib.predict_proba(Xte)\n",
    "    metrics_test = eval_report(yte, proba_te, calib.classes_)\n",
    "    print(\"AP macro (test):\", round(metrics_test[\"ap_macro\"], 4))\n",
    "    print(\"AP por clase (test):\", {k: round(v,4) for k,v in metrics_test[\"ap_per_class\"].items()})\n",
    "    print(\"Brier OvR medio (test):\", round(metrics_test[\"brier_ovr_mean\"], 5))\n",
    "    print(\"CM (test):\")\n",
    "    print(np.array(metrics_test[\"confusion_matrix\"]))\n",
    "\n",
    "    # guardar artefactos\n",
    "    base = f\"models/artifacts/{name}\"\n",
    "    joblib.dump(calib, f\"{base}_calibrated_isotonic.joblib\")\n",
    "    with open(f\"{base}_report_test.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics_test, f, indent=2, ensure_ascii=False)\n",
    "    print(\"Guardado:\", f\"{base}_calibrated_isotonic.joblib\")\n",
    "\n",
    "    # devolver cosas útiles (p.ej. para comparar ALL vs EU+USA)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"counts_train\": counts.to_dict(),\n",
    "        \"ap_macro_test\": metrics_test[\"ap_macro\"],\n",
    "        \"ap_per_class_test\": metrics_test[\"ap_per_class\"],\n",
    "        \"brier_test\": metrics_test[\"brier_ovr_mean\"],\n",
    "        \"cm_test\": metrics_test[\"confusion_matrix\"],\n",
    "    }\n",
    "\n",
    "# === 4) Ejecutar para ALL y EU+USA (23 features) ===\n",
    "res_all   = train_eval_one(\"data/processed/features/supervised_ALL.parquet\",   \"HGB_ALL_f23\")\n",
    "res_euusa = train_eval_one(\"data/processed/features/supervised_EUUSA.parquet\", \"HGB_EUUSA_f23\")\n",
    "\n",
    "res_all, res_euusa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92337334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mejor en VAL (macro-F1, th_r, th_b, margin) -> (0.3482834155100163, 0.55, 0.5, 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.6428571428571429, 0.2406847935548842, 0.6251402918069585],\n",
       " 'recall': [0.017786561264822136, 0.5945273631840796, 0.5626262626262626],\n",
       " 'f1': [0.03461538461538462, 0.3426523297491039, 0.5922381711855397],\n",
       " 'support': [506, 402, 990]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, joblib\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "\n",
    "def load_data(PQ, DF_PATH=\"data/interim/ES_5m_2021_2024.parquet\"):\n",
    "    D  = pd.read_parquet(PQ)               # X + target + idx + zone_type\n",
    "    df = pd.read_parquet(DF_PATH)\n",
    "    t  = df[\"Time\"].dt.tz_convert(\"Europe/Madrid\")\n",
    "    idx = D[\"idx\"].astype(int)\n",
    "    tr = (t.loc[idx] < \"2024-01-01\").values\n",
    "    va = (t.loc[idx] >= \"2024-01-01\").values & (t.loc[idx] < \"2024-07-01\").values\n",
    "    te = (t.loc[idx] >= \"2024-07-01\").values\n",
    "    X = D.drop(columns=[\"target\", \"idx\", \"zone_type\"], errors=\"ignore\")\n",
    "    y = D[\"target\"].astype(str).values\n",
    "    num = X.select_dtypes(include=[np.number]).columns\n",
    "    X[num] = X[num].fillna(0.0).astype(np.float32)\n",
    "    return X.to_numpy(), y, tr, va, te\n",
    "\n",
    "def tune_thresholds(calib_path, data_path):\n",
    "    X, y, tr, va, te = load_data(data_path)\n",
    "    model = joblib.load(calib_path)\n",
    "    classes = model.classes_\n",
    "    c2i = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "    Pva = model.predict_proba(X[va])\n",
    "    yva = y[va]\n",
    "\n",
    "    best = None\n",
    "    for th_r in np.linspace(0.55, 0.85, 16):      # umbral rebound\n",
    "      for th_b in np.linspace(0.50, 0.80, 16):    # umbral breakout\n",
    "        for margin in (0.00, 0.05, 0.10, 0.15):   # margen frente a la 2ª prob.\n",
    "          # regla:\n",
    "          #  - pred 'rebound' si p_r >= th_r y p_r - max(otros) >= margin\n",
    "          #  - pred 'breakout' si p_b >= th_b y p_b - max(otros) >= margin\n",
    "          #  - si ninguna dispara, pred 'none'\n",
    "          pr = Pva[:, c2i[\"rebound\"]]; pb = Pva[:, c2i[\"breakout\"]]; pn = Pva[:, c2i[\"none\"]]\n",
    "          top2 = np.sort(Pva, axis=1)[:, -2]\n",
    "          yhat = np.full(len(yva), \"none\", dtype=object)\n",
    "          yhat[(pr >= th_r) & (pr - top2 >= margin)] = \"rebound\"\n",
    "          yhat[(pb >= th_b) & (pb - top2 >= margin)] = \"breakout\"\n",
    "\n",
    "          # macro-F1 en validación\n",
    "          f1 = f1_score(yva, yhat, labels=[\"breakout\",\"none\",\"rebound\"], average=\"macro\")\n",
    "          if (best is None) or (f1 > best[0]):\n",
    "              best = (f1, th_r, th_b, margin)\n",
    "    return best  # (f1_macro, th_r, th_b, margin)\n",
    "\n",
    "def eval_with_thresholds(calib_path, data_path, th_r, th_b, margin):\n",
    "    X, y, tr, va, te = load_data(data_path)\n",
    "    model = joblib.load(calib_path)\n",
    "    classes = model.classes_\n",
    "    c2i = {c:i for i,c in enumerate(classes)}\n",
    "    Pte = model.predict_proba(X[te]); yte = y[te]\n",
    "    pr = Pte[:, c2i[\"rebound\"]]; pb = Pte[:, c2i[\"breakout\"]]\n",
    "    top2 = np.sort(Pte, axis=1)[:, -2]\n",
    "    yhat = np.full(len(yte), \"none\", dtype=object)\n",
    "    yhat[(pr >= th_r) & (pr - top2 >= margin)] = \"rebound\"\n",
    "    yhat[(pb >= th_b) & (pb - top2 >= margin)] = \"breakout\"\n",
    "    rep = precision_recall_fscore_support(yte, yhat, labels=[\"breakout\",\"none\",\"rebound\"], zero_division=0)\n",
    "    return {\"precision\":rep[0].tolist(),\"recall\":rep[1].tolist(),\"f1\":rep[2].tolist(),\"support\":rep[3].tolist()}\n",
    "\n",
    "# === aplica a EU+USA (recomendado) ===\n",
    "best = tune_thresholds(\"models/artifacts/HGB_EUUSA_f23_calibrated_isotonic.joblib\",\n",
    "                       \"data/processed/features/supervised_EUUSA.parquet\")\n",
    "print(\"mejor en VAL (macro-F1, th_r, th_b, margin) ->\", best)\n",
    "rep = eval_with_thresholds(\"models/artifacts/HGB_EUUSA_f23_calibrated_isotonic.joblib\",\n",
    "                           \"data/processed/features/supervised_EUUSA.parquet\",\n",
    "                           best[1], best[2], best[3])\n",
    "rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e34831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13180610889774239,\n",
       " 0.13180610889774239,\n",
       " 0.0,\n",
       " (0.5, 0.4, 0.65, 0.5, 0.1, 0.1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, joblib\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# === utilidades ya conocidas ===\n",
    "def load_data(PQ, DF_PATH=\"data/interim/ES_5m_2021_2024.parquet\"):\n",
    "    D  = pd.read_parquet(PQ)\n",
    "    df = pd.read_parquet(DF_PATH)\n",
    "    t  = df[\"Time\"].dt.tz_convert(\"Europe/Madrid\")\n",
    "    idx = D[\"idx\"].astype(int)\n",
    "    tr = (t.loc[idx] < \"2024-01-01\").values\n",
    "    va = (t.loc[idx] >= \"2024-01-01\").values & (t.loc[idx] < \"2024-07-01\").values\n",
    "    te = (t.loc[idx] >= \"2024-07-01\").values\n",
    "    X = D.drop(columns=[\"target\",\"idx\",\"zone_type\"], errors=\"ignore\")\n",
    "    y = D[\"target\"].astype(str).values\n",
    "    num = X.select_dtypes(include=[np.number]).columns\n",
    "    X[num] = X[num].fillna(0.0).astype(np.float32)\n",
    "    return X.to_numpy(), y, tr, va, te\n",
    "\n",
    "def decide_hier(P, classes, t_event, t_none, t_r, t_b, m_rb=0.0, m_br=0.0):\n",
    "    c2i = {c:i for i,c in enumerate(classes)}\n",
    "    pr, pb, pn = P[:,c2i[\"rebound\"]], P[:,c2i[\"breakout\"]], P[:,c2i[\"none\"]]\n",
    "    # 1) gate evento vs none\n",
    "    event_conf = np.maximum(pr, pb)\n",
    "    yhat = np.where((event_conf >= t_event) & (pn <= t_none), \"?\", \"none\")\n",
    "\n",
    "    # 2) dentro de evento, decidir r vs b con umbrales y márgenes\n",
    "    mask = (yhat == \"?\")\n",
    "    # márgenes uno-contra-uno\n",
    "    choose_r = (pr >= t_r) & (pr - pb >= m_rb)\n",
    "    choose_b = (pb >= t_b) & (pb - pr >= m_br)\n",
    "    yhat[mask & choose_r] = \"rebound\"\n",
    "    yhat[mask & (~choose_r) & choose_b] = \"breakout\"\n",
    "    yhat[mask & (~choose_r) & (~choose_b)] = \"none\"\n",
    "    return yhat\n",
    "\n",
    "# === búsqueda en validación (EU+USA) ===\n",
    "X, y, tr, va, te = load_data(\"data/processed/features/supervised_EUUSA.parquet\")\n",
    "model = joblib.load(\"models/artifacts/HGB_EUUSA_f23_calibrated_isotonic.joblib\")\n",
    "classes = model.classes_\n",
    "Pva = model.predict_proba(X[va]); yva = y[va]\n",
    "\n",
    "best = None\n",
    "for t_event in np.linspace(0.50, 0.75, 6):\n",
    "  for t_none in np.linspace(0.35, 0.55, 5):\n",
    "    for t_r in np.linspace(0.55, 0.75, 5):\n",
    "      for t_b in np.linspace(0.45, 0.70, 6):\n",
    "        for m_rb in (0.00, 0.05, 0.10):\n",
    "          for m_br in (0.00, 0.05, 0.10):\n",
    "            yhat = decide_hier(Pva, classes, t_event, t_none, t_r, t_b, m_rb, m_br)\n",
    "            P,R,F,S = precision_recall_fscore_support(\n",
    "                yva, yhat, labels=[\"breakout\",\"none\",\"rebound\"], zero_division=0\n",
    "            )\n",
    "            f1_macro = float(F.mean())\n",
    "            # opcional: priorizar recall de breakout\n",
    "            score = f1_macro + 0.05*R[0]\n",
    "            cand = (score, f1_macro, R[0], (t_event,t_none,t_r,t_b,m_rb,m_br))\n",
    "            if (best is None) or (cand > best):\n",
    "                best = cand\n",
    "\n",
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a4373",
   "metadata": {},
   "source": [
    "## Con Features de Order Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c91c4f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT  : c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\n",
      "FEATS : c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\data\\processed\\features\n",
      "EVENTS: c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\data\\processed\\events\n",
      "INTERM: c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\data\\interim\n",
      "Usando: supervised_EUUSA_EXT.parquet\n",
      "Shapes -> X: (8541, 29) | y: (8541,) | clases: ['breakout', 'none', 'rebound']\n"
     ]
    }
   ],
   "source": [
    "# --- Resolución robusta de rutas (independiente del CWD) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, joblib, json\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import average_precision_score, brier_score_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def find_repo_root(max_up=6):\n",
    "    p = Path.cwd()\n",
    "    for _ in range(max_up):\n",
    "        if (p/\"data\").exists() and (p/\"src\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path.cwd()  # fallback\n",
    "\n",
    "ROOT = find_repo_root()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "FEAT_DIR = DATA_DIR / \"processed\" / \"features\"\n",
    "EV_DIR = DATA_DIR / \"processed\" / \"events\"\n",
    "INT_DIR = DATA_DIR / \"interim\"\n",
    "\n",
    "print(\"ROOT  :\", ROOT)\n",
    "print(\"FEATS :\", FEAT_DIR)\n",
    "print(\"EVENTS:\", EV_DIR)\n",
    "print(\"INTERM:\", INT_DIR)\n",
    "\n",
    "# --- localizar dataset extendido (match: supervised_*_EXT.parquet) ---\n",
    "def find_ext():\n",
    "    cands = sorted(FEAT_DIR.glob(\"supervised_*_EXT.parquet\"))\n",
    "    if not cands:\n",
    "        # como en tu screenshot el nombre es exactamente supervised_EUUSA_EXT.parquet\n",
    "        p = FEAT_DIR / \"supervised_EUUSA_EXT.parquet\"\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise FileNotFoundError(f\"No encontré 'supervised_*_EXT.parquet' en {FEAT_DIR}\")\n",
    "    print(\"Usando:\", cands[0].name)\n",
    "    return cands[0]\n",
    "\n",
    "PQ = find_ext()\n",
    "DF_PATH = INT_DIR / \"ES_5m_2021_2024.parquet\"\n",
    "\n",
    "# --- carga datasets ---\n",
    "D  = pd.read_parquet(PQ)\n",
    "df = pd.read_parquet(DF_PATH)\n",
    "\n",
    "# split temporal (TR < 2024-01-01, VA 2024H1, TE 2024H2)\n",
    "t   = df[\"Time\"].dt.tz_convert(\"Europe/Madrid\")\n",
    "idx = D[\"idx\"].astype(int)\n",
    "TR  = (t.loc[idx] < \"2024-01-01\").values\n",
    "VA  = (t.loc[idx] >= \"2024-01-01\").values & (t.loc[idx] < \"2024-07-01\").values\n",
    "TE  = (t.loc[idx] >= \"2024-07-01\").values\n",
    "\n",
    "# X, y\n",
    "X = D.drop(columns=[\"target\",\"idx\",\"zone_type\"], errors=\"ignore\").copy()\n",
    "num = X.select_dtypes(include=[np.number]).columns\n",
    "X[num] = X[num].fillna(0.0).astype(np.float32)\n",
    "y = D[\"target\"].astype(str).values\n",
    "classes = np.array(sorted(np.unique(y))).tolist()\n",
    "print(\"Shapes -> X:\", X.shape, \"| y:\", y.shape, \"| clases:\", classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24bd8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HGB_EUUSA_EXT ===\n",
      "AP macro (test): 0.3561\n",
      "AP por clase: {'breakout': 0.3071, 'none': 0.2542, 'rebound': 0.507}\n",
      "Brier OvR medio (test): 0.21362\n",
      "Guardado: models/artifacts/HGB_EUUSA_EXT_calibrated_isotonic.joblib models/artifacts/HGB_EUUSA_EXT_report_test.json\n"
     ]
    }
   ],
   "source": [
    "# --- modelo base HGB (ligeramente regularizado) ---\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.08,\n",
    "    max_depth=None,\n",
    "    max_leaf_nodes=31,\n",
    "    min_samples_leaf=60,\n",
    "    l2_regularization=0.1,\n",
    "    class_weight=None,        # si breakout sigue flojo: prueba 'balanced'\n",
    "    random_state=42\n",
    ").fit(X[TR].to_numpy(), y[TR])\n",
    "\n",
    "# --- calibración isotónica one-vs-rest sobre VALIDACIÓN ---\n",
    "P_va = np.column_stack([hgb.predict_proba(X[VA].to_numpy())[:, list(hgb.classes_).index(c)] for c in classes])\n",
    "\n",
    "# entrenar un isotónico por clase (OvR)\n",
    "isos = {}\n",
    "P_va_cal = np.zeros_like(P_va)\n",
    "Y_va_bin = label_binarize(y[VA], classes=classes)\n",
    "for k, c in enumerate(classes):\n",
    "    ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    ir.fit(P_va[:,k], Y_va_bin[:,k])\n",
    "    isos[c] = ir\n",
    "    P_va_cal[:,k] = ir.transform(P_va[:,k])\n",
    "\n",
    "# función de predicción calibrada\n",
    "def predict_proba_cal(Xmat):\n",
    "    P = np.column_stack([hgb.predict_proba(Xmat)[:, list(hgb.classes_).index(c)] for c in classes])\n",
    "    Pcal = np.column_stack([isos[c].transform(P[:,k]) for k,c in enumerate(classes)])\n",
    "    # re-normaliza por fila para evitar drift numérico\n",
    "    s = Pcal.sum(axis=1, keepdims=True)\n",
    "    s[s==0] = 1.0\n",
    "    return Pcal / s\n",
    "\n",
    "# --- métricas en TEST ---\n",
    "P_te_cal = predict_proba_cal(X[TE].to_numpy())\n",
    "y_te = y[TE]\n",
    "\n",
    "# AP macro\n",
    "ap_macro = average_precision_score(label_binarize(y_te, classes=classes), P_te_cal, average=\"macro\")\n",
    "\n",
    "# Brier OvR medio\n",
    "brier = np.mean([brier_score_loss((y_te==c).astype(int), P_te_cal[:,i]) for i,c in enumerate(classes)])\n",
    "\n",
    "# AP por clase\n",
    "ap_per_class = {c: average_precision_score((y_te==c).astype(int), P_te_cal[:,i]) for i,c in enumerate(classes)}\n",
    "\n",
    "print(\"=== HGB_EUUSA_EXT ===\")\n",
    "print(\"AP macro (test):\", round(ap_macro,4))\n",
    "print(\"AP por clase:\", {k:round(v,4) for k,v in ap_per_class.items()})\n",
    "print(\"Brier OvR medio (test):\", round(brier,5))\n",
    "\n",
    "# guardar artefactos\n",
    "Path(\"models/artifacts\").mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump({\"model\":hgb,\"classes\":classes,\"isos\":isos}, \"models/artifacts/HGB_EUUSA_EXT_calibrated_isotonic.joblib\")\n",
    "\n",
    "json.dump({\n",
    "    \"name\":\"HGB_EUUSA_EXT\",\n",
    "    \"ap_macro_test\": float(ap_macro),\n",
    "    \"ap_per_class_test\": {k: float(v) for k,v in ap_per_class.items()},\n",
    "    \"brier_test\": float(brier),\n",
    "    \"n_features\": int(X.shape[1]),\n",
    "}, open(\"models/artifacts/HGB_EUUSA_EXT_report_test.json\",\"w\"), indent=2)\n",
    "\n",
    "print(\"Guardado:\",\n",
    "      \"models/artifacts/HGB_EUUSA_EXT_calibrated_isotonic.joblib\",\n",
    "      \"models/artifacts/HGB_EUUSA_EXT_report_test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e334562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD fijado a: c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\n"
     ]
    }
   ],
   "source": [
    "# opcional: fija CWD al raíz del repo (si el notebook vive en ./notebooks/)\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "repo_root = Path(__file__).resolve().parents[1] if \"__file__\" in globals() else Path.cwd().parents[0]\n",
    "os.chdir(repo_root)\n",
    "print(\"CWD fijado a:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3eb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:73: RuntimeWarning: Mean of empty slice.\n",
      "  imb_buy_strength_avg=float(buy_strength[buy_strength>0].mean() if buy_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:74: RuntimeWarning: Mean of empty slice.\n",
      "  imb_sell_strength_avg=float(sell_strength[sell_strength>0].mean() if sell_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:73: RuntimeWarning: Mean of empty slice.\n",
      "  imb_buy_strength_avg=float(buy_strength[buy_strength>0].mean() if buy_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:74: RuntimeWarning: Mean of empty slice.\n",
      "  imb_sell_strength_avg=float(sell_strength[sell_strength>0].mean() if sell_cnt>0 else 0.0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN cols: [] ... 0\n",
      "Const cols: [] ... 0\n",
      "base   | AP_macro=0.354 | AP_reb=0.495 | Brier=0.212\n",
      "mvc    | AP_macro=0.351 | AP_reb=0.495 | Brier=0.214\n",
      "of3x   | AP_macro=0.345 | AP_reb=0.482 | Brier=0.213\n",
      "ext3x  | AP_macro=0.357 | AP_reb=0.499 | Brier=0.213\n"
     ]
    }
   ],
   "source": [
    "# === 0) helpers de ruta ===EUUSA\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, joblib, json\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import average_precision_score, brier_score_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "for _ in range(6):\n",
    "    if (ROOT/\"data\").exists() and (ROOT/\"src\").exists(): break\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "DF_PATH = ROOT/\"data/interim/ES_5m_2021_2024.parquet\"\n",
    "EV_PATH = ROOT/\"data/processed/events/events_labeled_2021_2024.parquet\"   # ajusta si usas EUUSA\n",
    "print(\"ROOT:\", ROOT)\n",
    "\n",
    "# === 1) carga base y splits temporales ===\n",
    "df = pd.read_parquet(DF_PATH)\n",
    "events_labeled = pd.read_parquet(EV_PATH)\n",
    "\n",
    "t = df[\"Time\"].dt.tz_convert(\"Europe/Madrid\")\n",
    "def make_splits(idx):\n",
    "    TR = (t.loc[idx] < \"2024-01-01\").values\n",
    "    VA = (t.loc[idx] >= \"2024-01-01\").values & (t.loc[idx] < \"2024-07-01\").values\n",
    "    TE = (t.loc[idx] >= \"2024-07-01\").values\n",
    "    return TR, VA, TE\n",
    "\n",
    "# === 2) construir datasets on-the-fly para ablation ===\n",
    "from importlib import reload\n",
    "import ppz.pipelines.build_dataset as bd; reload(bd)\n",
    "\n",
    "def build_set(add_mvc=False, add_of=False, of_k=3.0):\n",
    "    X, y = bd.make_supervised_from_events(\n",
    "        df, events_labeled,\n",
    "        tick_size=0.25, n_short=20, n_long=60,\n",
    "        L_prev_touches=60, r_touch_ticks=6, drop_none=False,\n",
    "        add_mvc=add_mvc, add_orderflow=add_of, of_k=of_k, of_k_ext=5.0\n",
    "    )\n",
    "    num = X.select_dtypes(include=[np.number]).columns\n",
    "    X[num] = X[num].fillna(0.0).astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "sets = {\n",
    "    \"base\":  build_set(add_mvc=False, add_of=False),\n",
    "    \"mvc\":   build_set(add_mvc=True,  add_of=False),\n",
    "    \"of3x\":  build_set(add_mvc=False, add_of=True,  of_k=3.0),\n",
    "    \"ext3x\": build_set(add_mvc=True,  add_of=True,  of_k=3.0),\n",
    "}\n",
    "\n",
    "# === 3) sanity check rápido (NaNs, constantes) en EXT ===\n",
    "X_ext, y_ext = sets[\"ext3x\"]\n",
    "nan_cols = X_ext.columns[X_ext.isna().any()].tolist()\n",
    "const_cols = [c for c in X_ext.columns if X_ext[c].nunique(dropna=True)<=1]\n",
    "print(\"NaN cols:\", nan_cols[:10], \"...\", len(nan_cols))\n",
    "print(\"Const cols:\", const_cols[:10], \"...\", len(const_cols))\n",
    "\n",
    "# === 4) entrenamiento y métricas macro/por clase ===\n",
    "def train_eval(X, y, name=\"set\"):\n",
    "    idx = X[\"idx\"].astype(int).values\n",
    "    TR, VA, TE = make_splits(idx)\n",
    "    Xfit = X.drop(columns=[\"idx\",\"zone_type\"], errors=\"ignore\").copy()\n",
    "    y = y.astype(str).values\n",
    "    classes = sorted(np.unique(y).tolist())\n",
    "\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.06,\n",
    "        max_leaf_nodes=25,\n",
    "        min_samples_leaf=120,\n",
    "        l2_regularization=0.2,\n",
    "        random_state=42,\n",
    "    ).fit(Xfit[TR].to_numpy(), y[TR])\n",
    "\n",
    "    # isotónica OvR en VA\n",
    "    P_va = np.column_stack([hgb.predict_proba(Xfit[VA].to_numpy())[:, list(hgb.classes_).index(c)] for c in classes])\n",
    "    Y_va_bin = label_binarize(y[VA], classes=classes)\n",
    "    isos, P_va_cal = {}, np.zeros_like(P_va)\n",
    "    for k,c in enumerate(classes):\n",
    "        ir = IsotonicRegression(out_of_bounds=\"clip\").fit(P_va[:,k], Y_va_bin[:,k])\n",
    "        isos[c] = ir; P_va_cal[:,k] = ir.transform(P_va[:,k])\n",
    "\n",
    "    # test\n",
    "    def pred_cal(Xm):\n",
    "        P = np.column_stack([hgb.predict_proba(Xm)[:, list(hgb.classes_).index(c)] for c in classes])\n",
    "        Pcal = np.column_stack([isos[c].transform(P[:,k]) for k,c in enumerate(classes)])\n",
    "        s = Pcal.sum(axis=1, keepdims=True); s[s==0]=1.0\n",
    "        return Pcal/s\n",
    "\n",
    "    Xte = Xfit[TE].to_numpy(); yte = y[TE]\n",
    "    Pte = pred_cal(Xte)\n",
    "    ap_macro = average_precision_score(label_binarize(yte, classes=classes), Pte, average=\"macro\")\n",
    "    ap_pc = {c: average_precision_score((yte==c).astype(int), Pte[:,i]) for i,c in enumerate(classes)}\n",
    "    brier = np.mean([brier_score_loss((yte==c).astype(int), Pte[:,i]) for i,c in enumerate(classes)])\n",
    "\n",
    "    return {\"name\":name, \"ap_macro\":ap_macro, \"ap\":ap_pc, \"brier\":brier, \"model\":hgb, \"classes\":classes, \"Xfit\":Xfit, \"TR\":TR, \"VA\":VA, \"TE\":TE, \"y\":y}\n",
    "\n",
    "res = {k: train_eval(*v, name=k) for k,v in sets.items()}\n",
    "for k,v in res.items():\n",
    "    print(f\"{k:6s} | AP_macro={v['ap_macro']:.3f} | AP_reb={v['ap'].get('rebound',np.nan):.3f} | Brier={v['brier']:.3f}\")\n",
    "\n",
    "# === 5) (opcional) permutation importance en VALIDACIÓN para EXT, target=rebote (brier-like) ===\n",
    "# OJO: puede tardar; reduce n_repeats si va lento.\n",
    "from sklearn.metrics import log_loss\n",
    "def perm_importance_rebound(v, n_repeats=5, max_feats=30):\n",
    "    model, Xfit, VA, y, classes = v[\"model\"], v[\"Xfit\"], v[\"VA\"], v[\"y\"], v[\"classes\"]\n",
    "    Xv = Xfit[VA]; yv = (y[VA]==\"rebound\").astype(int)\n",
    "    # usa proba(rebound) del modelo calibrado aproximando con predict_proba sin isotónica (proxy)\n",
    "    pr = model.predict_proba(Xv.to_numpy())[:, list(model.classes_).index(\"rebound\")]\n",
    "    base = log_loss(yv, np.clip(pr,1e-6,1-1e-6))\n",
    "    cols = Xv.columns.tolist()\n",
    "    rng = np.random.default_rng(42)\n",
    "    scores=[]\n",
    "    for _ in range(n_repeats):\n",
    "        for c in cols[:max_feats]:\n",
    "            Xp = Xv.copy()\n",
    "            Xp[c] = rng.permutation(Xp[c].values)\n",
    "            pr_p = model.predict_proba(Xp.to_numpy())[:, list(model.classes_).index(\"rebound\")]\n",
    "            scores.append((c, base - log_loss(yv, np.clip(pr_p,1e-6,1-1e-6))))\n",
    "    imp = pd.DataFrame(scores, columns=[\"feature\",\"gain\"]).groupby(\"feature\")[\"gain\"].mean().sort_values(ascending=False)\n",
    "    return imp\n",
    "\n",
    "# imp = perm_importance_rebound(res[\"ext3x\"]) ; imp.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b4b3258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:74: RuntimeWarning: Mean of empty slice.\n",
      "  imb_sell_strength_avg=float(sell_strength[sell_strength>0].mean() if sell_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:73: RuntimeWarning: Mean of empty slice.\n",
      "  imb_buy_strength_avg=float(buy_strength[buy_strength>0].mean() if buy_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:74: RuntimeWarning: Mean of empty slice.\n",
      "  imb_sell_strength_avg=float(sell_strength[sell_strength>0].mean() if sell_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:73: RuntimeWarning: Mean of empty slice.\n",
      "  imb_buy_strength_avg=float(buy_strength[buy_strength>0].mean() if buy_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:73: RuntimeWarning: Mean of empty slice.\n",
      "  imb_buy_strength_avg=float(buy_strength[buy_strength>0].mean() if buy_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:74: RuntimeWarning: Mean of empty slice.\n",
      "  imb_sell_strength_avg=float(sell_strength[sell_strength>0].mean() if sell_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:73: RuntimeWarning: Mean of empty slice.\n",
      "  imb_buy_strength_avg=float(buy_strength[buy_strength>0].mean() if buy_cnt>0 else 0.0),\n",
      "c:\\Users\\jmbf2\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\features\\of_imbalance.py:74: RuntimeWarning: Mean of empty slice.\n",
      "  imb_sell_strength_avg=float(sell_strength[sell_strength>0].mean() if sell_cnt>0 else 0.0),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>k_ext</th>\n",
       "      <th>AP_macro</th>\n",
       "      <th>AP_rebound</th>\n",
       "      <th>Brier</th>\n",
       "      <th>n_feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.360059</td>\n",
       "      <td>0.519927</td>\n",
       "      <td>0.212116</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.357398</td>\n",
       "      <td>0.498636</td>\n",
       "      <td>0.212673</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.357040</td>\n",
       "      <td>0.495235</td>\n",
       "      <td>0.212117</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.358506</td>\n",
       "      <td>0.494386</td>\n",
       "      <td>0.213423</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k  k_ext  AP_macro  AP_rebound     Brier  n_feat\n",
       "1  2.5    4.5  0.360059    0.519927  0.212116      31\n",
       "2  3.0    5.0  0.357398    0.498636  0.212673      31\n",
       "3  3.5    5.5  0.357040    0.495235  0.212117      31\n",
       "0  2.0    4.0  0.358506    0.494386  0.213423      31"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mini grid de OF: k y k_ext\n",
    "grid = [(2.0,4.0),(2.5,4.5),(3.0,5.0),(3.5,5.5)]\n",
    "results = []\n",
    "for k, kext in grid:\n",
    "    Xk, yk = bd.make_supervised_from_events(\n",
    "        df, events_labeled,\n",
    "        tick_size=0.25, n_short=20, n_long=60,\n",
    "        L_prev_touches=60, r_touch_ticks=6, drop_none=False,\n",
    "        add_mvc=True, add_orderflow=True, of_k=k, of_k_ext=kext\n",
    "    )\n",
    "    num = Xk.select_dtypes(include=[np.number]).columns\n",
    "    Xk[num] = Xk[num].fillna(0.0).astype(np.float32)\n",
    "    r = train_eval(Xk, yk, name=f\"ext k={k}\")\n",
    "    results.append((k, kext, r[\"ap_macro\"], r[\"ap\"].get(\"rebound\", np.nan), r[\"brier\"], Xk.shape[1]))\n",
    "\n",
    "pd.DataFrame(results, columns=[\"k\",\"k_ext\",\"AP_macro\",\"AP_rebound\",\"Brier\",\"n_feat\"]).sort_values(\"AP_rebound\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f5e3c",
   "metadata": {},
   "source": [
    "## Refino del entreno con MVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7787b3d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'make_supervised_from_events' from partially initialized module 'ppz.pipelines.build_dataset' (most likely due to a circular import) (c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\pipelines\\build_dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../src\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mppz\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_dataset \u001b[38;5;28;01mas\u001b[39;00m bd  \u001b[38;5;66;03m# el archivo anterior\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mppz\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmvc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MvcFlagsParams\n\u001b[0;32m     10\u001b[0m ROOT \u001b[38;5;241m=\u001b[39m Path\u001b[38;5;241m.\u001b[39mcwd()\n",
      "File \u001b[1;32mc:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\pipelines\\build_dataset.py:14\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mppz\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mof_imbalance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_of_features_at\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Importa tu builder base existente\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Debe existir make_supervised_from_events(df, events, **kwargs) -> (X, y)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# con X conteniendo al menos 'idx' y 'zone_type'\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_supervised_from_events \u001b[38;5;28;01mas\u001b[39;00m _base_builder  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mppz\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmvc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotate_mvc_directional_flags, MvcFlagsParams\n\u001b[0;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_supervised_from_events\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'make_supervised_from_events' from partially initialized module 'ppz.pipelines.build_dataset' (most likely due to a circular import) (c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\../src\\ppz\\pipelines\\build_dataset.py)"
     ]
    }
   ],
   "source": [
    "# === Construcción (o carga) del dataset MVC-dir EU+USA ===\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, json, joblib\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from ppz.pipelines import build_dataset as bd  # el archivo anterior\n",
    "from ppz.features.mvc import MvcFlagsParams\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "for _ in range(6):\n",
    "    if (ROOT/\"data\").exists() and (ROOT/\"src\").exists(): break\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "DF_PATH = ROOT/\"data/interim/ES_5m_2021_2024.parquet\"\n",
    "EV_PATH = ROOT/\"data/processed/events/events_labeled_2021_2024.parquet\"\n",
    "OUT_FEAT = ROOT/\"data/processed/features/supervised_EUUSA_MVCdir.parquet\"\n",
    "\n",
    "df = pd.read_parquet(DF_PATH)\n",
    "ev = pd.read_parquet(EV_PATH)\n",
    "\n",
    "# Build (si no existe) o carga\n",
    "if not OUT_FEAT.exists():\n",
    "    X_mvc, y_mvc = bd.make_supervised_from_events_mvcdir(\n",
    "        df, ev,\n",
    "        mvc_params=MvcFlagsParams(tick_size=0.25, mvc_lower=1/3, mvc_upper=2/3,\n",
    "                                  vwap_slope_window=20, vwap_flat_th_ticks_per_bar=0.05,\n",
    "                                  first_touch_static=True),\n",
    "        subset_sessions=(\"EU\",\"USA\"),\n",
    "        tick_size=0.25, n_short=20, n_long=60,\n",
    "        L_prev_touches=60, r_touch_ticks=6,\n",
    "        drop_none=False\n",
    "    )\n",
    "    bd.save_supervised_mvcdir(X_mvc, y_mvc, OUT_FEAT)\n",
    "\n",
    "D  = pd.read_parquet(OUT_FEAT)\n",
    "df = pd.read_parquet(DF_PATH)  # para cortes temporales\n",
    "\n",
    "# === Split temporal: TR < 2024-01-01, VA 2024H1, TE 2024H2 ===\n",
    "t   = df[\"Time\"].dt.tz_convert(\"Europe/Madrid\")\n",
    "idx = D[\"idx\"].astype(int)\n",
    "TR  = (t.loc[idx] < \"2024-01-01\").values\n",
    "VA  = (t.loc[idx] >= \"2024-01-01\").values & (t.loc[idx] < \"2024-07-01\").values\n",
    "TE  = (t.loc[idx] >= \"2024-07-01\").values\n",
    "\n",
    "# === X / y binario: rebound vs no-rebound ===\n",
    "y_str = D[\"target\"].astype(str).values\n",
    "y_bin = (y_str == \"rebound\").astype(int)\n",
    "\n",
    "# Drop columnas no predictoras\n",
    "X = D.drop(columns=[\"target\",\"idx\",\"zone_type\"], errors=\"ignore\").copy()\n",
    "num = X.select_dtypes(include=[np.number]).columns\n",
    "X[num] = X[num].fillna(0.0).astype(np.float32)\n",
    "\n",
    "Xtr, ytr = X.iloc[TR].to_numpy(), y_bin[TR]\n",
    "Xva, yva = X.iloc[VA].to_numpy(), y_bin[VA]\n",
    "Xte, yte = X.iloc[TE].to_numpy(), y_bin[TE]\n",
    "\n",
    "# === Modelo base HGB binario ===\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import average_precision_score, brier_score_loss, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.08,\n",
    "    max_leaf_nodes=31,\n",
    "    min_samples_leaf=60,\n",
    "    l2_regularization=0.1,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ").fit(Xtr, ytr)\n",
    "\n",
    "# === Calibración isotónica (en VALIDACIÓN) ===\n",
    "p_va = hgb.predict_proba(Xva)[:,1]\n",
    "iso  = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va, yva)\n",
    "\n",
    "def predict_proba_cal(Xm):\n",
    "    p = hgb.predict_proba(Xm)[:,1]\n",
    "    return iso.transform(p)\n",
    "\n",
    "# === Métricas ===\n",
    "p_te_cal = predict_proba_cal(Xte)\n",
    "\n",
    "ap_reb   = average_precision_score(yte, p_te_cal)\n",
    "brier    = brier_score_loss(yte, p_te_cal)\n",
    "# Umbral por F1 en VALIDACIÓN (grid sencillo)\n",
    "ths = np.linspace(0.4, 0.8, 9)\n",
    "f1_va, th_best = -1.0, 0.6\n",
    "p_va_cal = predict_proba_cal(Xva)\n",
    "for th in ths:\n",
    "    yhat = (p_va_cal >= th).astype(int)\n",
    "    _, _, f1, _ = precision_recall_fscore_support(yva, yhat, average=\"binary\", zero_division=0)\n",
    "    if f1 > f1_va:\n",
    "        f1_va, th_best = f1, th\n",
    "\n",
    "yhat_te = (p_te_cal >= th_best).astype(int)\n",
    "prec, rec, f1, sup = precision_recall_fscore_support(yte, yhat_te, average=None, labels=[1,0], zero_division=0)\n",
    "cm = confusion_matrix(yte, yhat_te, labels=[1,0])\n",
    "\n",
    "print(\"=== Rebound binario (EU+USA, MVC-dir) ===\")\n",
    "print(f\"AP_rebound (test): {ap_reb:.4f}\")\n",
    "print(f\"Brier (test):      {brier:.5f}\")\n",
    "print(f\"Best th (VAL):     {th_best:.2f}\")\n",
    "print(\"CM (test) [rows=true, cols=pred] [rebound, not]:\")\n",
    "print(cm)\n",
    "\n",
    "# === Guarda artefactos ===\n",
    "ART_DIR = ROOT/\"models/artifacts\"; ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump({\"model\":hgb, \"iso\":iso, \"th_best\":float(th_best)}, ART_DIR/\"HGB_rebound_EUUSA_MVCdir.joblib\")\n",
    "json.dump({\n",
    "    \"name\":\"HGB_rebound_EUUSA_MVCdir\",\n",
    "    \"ap_rebound_test\": float(ap_reb),\n",
    "    \"brier_test\": float(brier),\n",
    "    \"th_best_val\": float(th_best),\n",
    "    \"cm_test\": cm.tolist()\n",
    "}, open(ART_DIR/\"HGB_rebound_EUUSA_MVCdir_report.json\",\"w\"), indent=2)\n",
    "print(\"Guardado:\", str(ART_DIR/\"HGB_rebound_EUUSA_MVCdir.joblib\"), str(ART_DIR/\"HGB_rebound_EUUSA_MVCdir_report.json\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
