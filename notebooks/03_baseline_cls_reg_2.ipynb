{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459e87d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: c:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\n",
      "Events dir: C:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\notebooks\\data\\processed\\events\n",
      "Archivos disponibles:\n",
      " - events_labeled_2021_2024_ALL.parquet\n",
      " - events_labeled_2021_2024_EU.parquet\n",
      " - events_labeled_2021_2024_EUUSA.parquet\n",
      " - events_labeled_2021_2024_USA.parquet\n",
      "Usando candidato: events_labeled_2021_2024_ALL.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Detecta la raíz del repo (sube directorios hasta encontrar /data)\n",
    "ROOT = Path.cwd()\n",
    "while not (ROOT / \"data\").exists() and ROOT.parent != ROOT:\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "EV_DIR = ROOT / \"data\" / \"processed\" / \"events\"\n",
    "print(\"Events dir:\", EV_DIR.resolve())\n",
    "\n",
    "# 2) Lista lo que hay (por si el nombre difiere)\n",
    "print(\"Archivos disponibles:\")\n",
    "for p in sorted(EV_DIR.glob(\"events_labeled_*.parquet\")):\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "# 3) Carga robusta: si existe el exacto, úsalo; si no, coge el primero que matchea\n",
    "target = EV_DIR / \"events_labeled_2021_2024.parquet\"\n",
    "if not target.exists():\n",
    "    cands = sorted(EV_DIR.glob(\"events_labeled_2021_2024*.parquet\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"No hay ningún 'events_labeled_2021_2024*.parquet' en \" + str(EV_DIR))\n",
    "    target = cands[0]\n",
    "    print(\"Usando candidato:\", target.name)\n",
    "\n",
    "ev_all = pd.read_parquet(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe29fcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'events_path' from 'ppz.io.paths' (C:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\src\\ppz\\io\\paths.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 1) Carga base\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mppz\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m events_path, interim_path\n\u001b[0;32m      8\u001b[0m df     \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(interim_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mES_5m_2021_2024.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      9\u001b[0m ev_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(events_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevents_labeled_2021_2024.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'events_path' from 'ppz.io.paths' (C:\\Users\\jmbf2\\OneDrive\\Trading\\Machine Learning\\ZoneBasedPricePrediction\\src\\ppz\\io\\paths.py)"
     ]
    }
   ],
   "source": [
    "# === Re-train baseline con condiciones MVC + primer toque estáticas + slope VWAP ===\n",
    "import numpy as np, pandas as pd, json, joblib\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# 1) Carga base\n",
    "from ppz.io.paths import events_path, interim_path\n",
    "df     = pd.read_parquet(interim_path(\"ES_5m_2021_2024.parquet\"))\n",
    "ev_all = pd.read_parquet(events_path(\"events_labeled_2021_2024.parquet\"))\n",
    "\n",
    "\n",
    "# 2) Filtro “primer toque” para zonas estáticas (VWAP no se filtra)\n",
    "STATIC_ZONES = {\"PDH_prev\",\"PDL_prev\",\"VAH_D1\",\"POC_D1\",\"VAL_D1\",\"USA_IBH\",\"USA_IBL\"}\n",
    "\n",
    "ev_sorted = ev_all.sort_values([\"session_id\",\"idx\"]).reset_index(drop=True)\n",
    "mask_static  = ev_sorted[\"zone_type\"].isin(STATIC_ZONES)\n",
    "mask_vwap    = ev_sorted[\"zone_type\"].eq(\"VWAP\")\n",
    "\n",
    "# primer toque por (session_id, zone_type)\n",
    "first_touch = (\n",
    "    ev_sorted[mask_static]\n",
    "      .groupby([\"session_id\",\"zone_type\"], as_index=False)\n",
    "      .head(1)\n",
    "      .assign(_keep=True)[[\"idx\",\"_keep\"]]\n",
    ")\n",
    "\n",
    "ev_ft = ev_sorted.merge(first_touch, on=\"idx\", how=\"left\")\n",
    "ev_ft[\"_keep\"] = ev_ft[\"_keep\"].fillna(False)\n",
    "ev_ft = pd.concat([\n",
    "    ev_ft[mask_vwap],           # todos los VWAP\n",
    "    ev_ft[mask_static & ev_ft[\"_keep\"]]  # primer toque en estáticas\n",
    "], axis=0).sort_values(\"idx\").reset_index(drop=True)\n",
    "ev_ft = ev_ft.drop(columns=[\"_keep\"])\n",
    "\n",
    "print(\"Eventos totales:\", len(ev_all), \"| Elegibles tras condición:\", len(ev_ft))\n",
    "\n",
    "# 3) Dataset supervisado con MVC features\n",
    "import ppz.pipelines.build_dataset as bd; reload(bd)\n",
    "\n",
    "X, y = bd.make_supervised_from_events(\n",
    "    df, ev_ft,\n",
    "    tick_size=0.25, n_short=20, n_long=60,\n",
    "    L_prev_touches=60, r_touch_ticks=6,\n",
    "    drop_none=False,          # mantenemos las 3 clases para referencia\n",
    "    add_mvc=True,             # << MVC activado\n",
    "    add_orderflow=False       # OF off por ahora\n",
    ")\n",
    "\n",
    "# 4) Añadir pendiente de VWAP (ticks/20 velas) como feature extra\n",
    "N_SLOPE = 20\n",
    "vwap = df[\"VWAP\"].to_numpy(dtype=float)\n",
    "slope = np.empty_like(vwap)\n",
    "slope[:] = np.nan\n",
    "slope[N_SLOPE:] = (vwap[N_SLOPE:] - vwap[:-N_SLOPE]) / (N_SLOPE * 0.25)  # ticks por vela\n",
    "slope = pd.Series(slope)\n",
    "\n",
    "idx_evt = X[\"idx\"].astype(int).values\n",
    "X[\"vwap_slope_n20\"] = slope.iloc[idx_evt].astype(np.float32).fillna(0.0)\n",
    "X[\"is_vwap_event\"]  = ev_ft[\"zone_type\"].eq(\"VWAP\").astype(np.int8).values\n",
    "\n",
    "# 5) Tipos compactos y guardado del dataset (para reproducibilidad)\n",
    "num = X.select_dtypes(include=[np.number]).columns\n",
    "X[num] = X[num].astype(np.float32, errors=\"ignore\")\n",
    "Path(\"data/processed/features\").mkdir(parents=True, exist_ok=True)\n",
    "Ds = X.copy(); Ds[\"target\"] = y\n",
    "Ds.to_parquet(\"data/processed/features/supervised_MVC_FIRSTTOUCH.parquet\",\n",
    "              engine=\"pyarrow\", compression=\"zstd\", index=False)\n",
    "\n",
    "# 6) Split temporal TR/VA/TE\n",
    "t = df[\"Time\"].dt.tz_convert(\"Europe/Madrid\")\n",
    "TR = (t.loc[idx_evt] < \"2024-01-01\").values\n",
    "VA = (t.loc[idx_evt] >= \"2024-01-01\").values & (t.loc[idx_evt] < \"2024-07-01\").values\n",
    "TE = (t.loc[idx_evt] >= \"2024-07-01\").values\n",
    "\n",
    "# 7) Entrenamiento HGB + calibración isotónica OvR\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import average_precision_score, brier_score_loss, confusion_matrix\n",
    "\n",
    "Xtr = X.drop(columns=[\"idx\",\"zone_type\"], errors=\"ignore\").iloc[TR].to_numpy()\n",
    "Xva = X.drop(columns=[\"idx\",\"zone_type\"], errors=\"ignore\").iloc[VA].to_numpy()\n",
    "Xte = X.drop(columns=[\"idx\",\"zone_type\"], errors=\"ignore\").iloc[TE].to_numpy()\n",
    "\n",
    "y_arr = Ds[\"target\"].astype(str).values\n",
    "ytr, yva, yte = y_arr[TR], y_arr[VA], y_arr[TE]\n",
    "classes = sorted(np.unique(y_arr))\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.08,\n",
    "    max_leaf_nodes=31,\n",
    "    min_samples_leaf=60,\n",
    "    l2_regularization=0.1,\n",
    "    class_weight=None,     # si 'breakout' queda muy flojo, prueba 'balanced'\n",
    "    random_state=42\n",
    ").fit(Xtr, ytr)\n",
    "\n",
    "# calibración OvR\n",
    "def proba_mat(model, Xmat, classes):\n",
    "    cols = [list(model.classes_).index(c) for c in classes]\n",
    "    return np.column_stack([model.predict_proba(Xmat)[:,i] for i in cols])\n",
    "\n",
    "P_va = proba_mat(hgb, Xva, classes)\n",
    "Y_va_bin = label_binarize(yva, classes=classes)\n",
    "isos = {}\n",
    "P_va_cal = np.zeros_like(P_va)\n",
    "for k,c in enumerate(classes):\n",
    "    ir = IsotonicRegression(out_of_bounds=\"clip\").fit(P_va[:,k], Y_va_bin[:,k])\n",
    "    isos[c] = ir\n",
    "    P_va_cal[:,k] = ir.transform(P_va[:,k])\n",
    "\n",
    "def predict_proba_cal(Xmat):\n",
    "    P = proba_mat(hgb, Xmat, classes)\n",
    "    Pcal = np.column_stack([isos[c].transform(P[:,k]) for k,c in enumerate(classes)])\n",
    "    s = Pcal.sum(axis=1, keepdims=True); s[s==0]=1.0\n",
    "    return Pcal/s\n",
    "\n",
    "# 8) Métricas en TEST (multiclase)\n",
    "P_te = predict_proba_cal(Xte)\n",
    "ap_macro = average_precision_score(label_binarize(yte, classes=classes), P_te, average=\"macro\")\n",
    "ap_per_class = {c: average_precision_score((yte==c).astype(int), P_te[:,i]) for i,c in enumerate(classes)}\n",
    "brier = np.mean([brier_score_loss((yte==c).astype(int), P_te[:,i]) for i,c in enumerate(classes)])\n",
    "y_pred = np.array(classes)[P_te.argmax(1)]\n",
    "cm = confusion_matrix(yte, y_pred, labels=classes).tolist()\n",
    "\n",
    "print(\"=== HGB_MVC_FIRSTTOUCH ===\")\n",
    "print(\"AP macro (test):\", round(ap_macro,4))\n",
    "print(\"AP por clase:\", {k: round(v,4) for k,v in ap_per_class.items()})\n",
    "print(\"Brier OvR medio (test):\", round(brier,5))\n",
    "print(\"CM (test) [rows=true, cols=pred] (classes={}):\".format(classes), cm)\n",
    "\n",
    "# 9) Guardar artefactos\n",
    "Path(\"models/artifacts\").mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump({\"model\":hgb,\"classes\":classes,\"isos\":isos},\n",
    "            \"models/artifacts/HGB_MVC_FIRSTTOUCH_calibrated.joblib\")\n",
    "json.dump({\n",
    "    \"name\":\"HGB_MVC_FIRSTTOUCH\",\n",
    "    \"ap_macro_test\": float(ap_macro),\n",
    "    \"ap_per_class_test\": {k: float(v) for k,v in ap_per_class.items()},\n",
    "    \"brier_test\": float(brier),\n",
    "    \"classes\": classes\n",
    "}, open(\"models/artifacts/HGB_MVC_FIRSTTOUCH_report_test.json\",\"w\"), indent=2)\n",
    "print(\"Guardado artefactos en models/artifacts/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
